<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="concept_ydh_lmj_vs2">
 
 <title>
  Cluster Manager
 </title>
 
 <shortdesc>
  The Couchbase <i>Cluster Manager</i> runs on all the nodes of a cluster, 
  orchestrating cluster-wide operations.
 </shortdesc>
 
 <body>
  
  <section>
  
   <title>
    Cluster Manager Architecture
   </title>
   
   <p>
    The architecture of the Cluster Manager is as follows:
   </p>
  
   <p>
    <image href="./images/clusterManagerArchitecture.png"  width="600" id="cluster_manager_architecture"/>
   </p>
   
   <p>
    As the above illustration shows, the Cluster Manager consists of two processes. the first Cluster Manager, the
    <i>babysitter</i>, is responsible for maintaining a variety of Couchbase Server-processes,
    these include the second Cluster Manager process, <i>ns-server</i>. The <i>babysitter</i> starts and monitors
    all the processes, logging their output to the file <codeph>babysitter.log</codeph>. If any of the
    processes dies, the <i>babysitter</i> restarts it. The <i>babysitter</i> is not cluster-aware.
   </p>
   
   <p>
    The processes for which the <i>babysitter</i> is responsible are:
   </p>
   
   <ul>
    <li>
     <i>ns-server</i>: Manages the node's participation in the cluster, as described below.
     <p>
      
     </p>
    </li>
    
    <li>
     <i>kv engine</i>: Runs as part of the 
     <xref href="../services-and-indexes/services/data-service.dita" scope="local" format="dita">Data Service</xref>, which
     must be installed on at least one cluster-node. Provides access to 
     <xref href="../data/data.dita" scope="local" format="dita">Data</xref>.
     <p>
      
     </p>
    </li>
    
    <li>
     <i>services</i>: One or more Couchbase 
     <xref href="../services-and-indexes/services/services.dita" scope="local" format="dita">Services</xref> that
     optionally run on the node.
     <p>
      
     </p>
    </li>
    
    <li>
     <i>xdcr</i>: The program for handling <i>Cross Data-Center Replication</i> (XDCR). This is always installed, but
     runs as an OS-level process, separate from the Cluster Manager. See
     <xref href="../clusters-and-availability/replication-architecture.dita" scope="local" format="dita">Availability</xref>,
     for information.
     <p>
      
     </p>
    </li>
    
    <li>
     <i>view engine</i>: The program for handling <i>Views</i>. This is always installed, but
     runs as an OS-level process, separate from the Cluster Manager. See
     <xref href="../views/views-intro.dita" scope="local" format="dita">Views</xref>, for more
     information.
     <p>
      
     </p>
    </li>
    
    <li>
     <i>other</i>: Various ancillary programs.
     <p>
      
     </p>
    </li>
    
   </ul>
   
  </section>
  
  <section>
   
   <title>
    ns-server
   </title>
   
   <p>
    The principal Cluster-Manager process is <i>ns-server</i>, whose architecture is as follows:
   </p>
   
   <p>
    <image href="./images/nsServerArchitecture.png"  width="600" id="ns_server_architecture"/>
   </p>
   

   <p> 
    The modules are: 
   </p>
   
   <p>
    
   </p>
     
   <ul>
    
   <li>
    <i>REST Administration (UI &amp; CLI)</i>: Supports administration of Couchbase Server, by means
    of a REST API; which itself underlies both the user interface
    provided by <i>Couchbase Web Console</i>, and the <i>Couchbase Command-Line Interface</i>.
    
    <p>
     
    </p>
   
   </li>
    
    <li>
     <i>Authentication</i>: Provides <i>Role-Based Access Control</i> for resources on the node. This is based
     on credentials (usernames and passwords) associated with system-defined <i>roles</i>, each
     of which is associated with a range of privileges. For detailed information, see
     <xref href="../../security/security-authorization.dita" scope="local" format="dita">Authorization</xref>.
     
     <p>
      
     </p>
     
    </li>
    
   <li>
    <i>Master Services</i>: Manages cluster-wide operations; such as master and replica 
    vBucket-placement, failover, node addition and subtraction, rebalance, cluster configuration-mapping, and 
    aggregation of statistical data. Note 
    that at a given, only one of the instances of <i>Master Services</i>
    on a multi-node cluster is in charge: the instances having negotiated among themselves, to identify and elect the
    instance. Should the elected instance subsequently become unavailable, another takes over.
    
    <p>
     
    </p>
   
   </li>
    
    <li>
     <b>Per Node Services</b>: Manages the health of the current node, and handles
     the monitoring and restarting of its processes and services.
     
     <p>
      
     </p>
     
    </li>
    
   <li>
    <b>Per Node Bucket Services</b>: Manages bucket-level operations for the current node; supporting replication,
    fail-over, restart, and statistics' collection. 
    
    <p>
     
    </p>
   
   </li>
    
   
    
   <li>
    <b>Generic Local Facilities</b>: Provides local 
    configuration management, libraries, workqueues, logging, clocks, ids, and events.
    
    <p>
     
    </p>
   
   </li>
    
    <li>
     <b>Generic Distributed Facilities</b>: Supports node-discovery, configuration messaging and alerts,
     replication, and heartbeat-transmission.
     
     <p>
      
     </p>
     
    </li>
  </ul>
 
  </section>

  <section>
   <title>
    Adding and Removing Nodes
   </title>
   
   <p>
    The Cluster Manager is responsible for cluster membership. When topology changes,
    the Cluster Manager walks through a set of carefully orchestrated operations, 
    to redistribute the load while keeping the existing workload running.
   </p>
   
   <p>
    The following sequence describes the high-level operations whereby a new node
    running the Data Service is added to the cluster;
   </p>
   
   <p>
   </p>
    
    <ol>
     
    <li>
     The Cluster Manager updates the new nodes with the existing cluster
     configuration.
     
     <p>
      
     </p>
    
    </li>
     
    <li>
     The Cluster Manager initiates rebalance and recalculates the vBucket map. 
     
     <p>
      
     </p>
    
    </li>
     
    <li>
     The nodes that are to receive data initiate DCP replication-streams from the existing nodes for 
     each vBucket, and begin building new copies of those vBuckets.  This occurs for both active and 
     replica vBuckets, depending on the new vBucket map layout.
     
     <p>
      
     </p>
    
    </li>
     
    <li>
     Incrementally, as each new vBucket is populated, the data is replicated, and indexes are  
     updated, an <i>atomic switchover</i> takes place, from the old vBucket to the new vBucket.
     
     <p>
      
     </p>
    
    </li>
     
    <li>
     As the new vBuckets on the new nodes become active, the Cluster Manager ensures that the new vBucket 
     map and cluster topology are communicated to all the existing nodes and clients. This process is repeated 
     until the rebalance operation completes running. 
     
     <p>
      
     </p>
    
    </li>
   </ol> 

   <p>
    Removal of one or more nodes from the data service follows a similar process; by creating new 
    vBuckets within the remaining nodes of the cluster, and transitioning them off the nodes that are to 
    be removed. When no more vBuckets remain assigned to a node, the node is removed from the cluster. 
   </p>
   
   <p>
    When adding or removing nodes that do not host the Data Service, no data is moved; and so the new
    nodes are simply added or removed from the cluster map. Client SDKs automatically begin load balancing 
    across those services, using the new cluster map.
   </p>
   
  </section>
  
  <section id="section_sv4_jgz_pz">
   
   <title>Node-Failure Detection</title>
   
   <p>
    Nodes within a
    Couchbase Server-cluster provide status on their health by means of a <i>heartbeat</i> mechanism.
    Heartbeats are provided by all instances of the Cluster Manager, at regular
    intervals. Each heartbeat contains basic statistics on the node, which are used to assess the
    node's condition.
   </p>
   
   <p dir="ltr">
    For the purposes of auto-failover, one node is
    designated as the <i>orchestrator</i> based on its election by the other nodes. This orchestrator is
    responsible for keeping track of heartbeats received from all other nodes. If automatic
    failover is enabled, after a period of time given by the automatic failover
    timeout period, if no heartbeats have been received from a particular node, the orchestrator node 
    automatically fails the node over. This is dependent on other criteria being met as detailed on
     <xref href="../../clustersetup/automatic-failover.dita#topic_fcf_chm_zs"/>, such as only a single
    node being unhealthy.
   </p>
   
   <p dir="ltr">
    This approach is fairly brittle and susceptible to 'false
    positives', that is a node being identified as offline even if the service on that node (e.g.
    Query, Data etc) continues to serve traffic without issue. Additionally, as the orchestrator is
    solely responsible for deciding which nodes to failover it is vulnerable to issues such as
    network partitions. </p><p dir="ltr">To mitigate these issues, the monitoring of node health is
    performed by a node monitor, which exists outside of the cluster management process, rather than
    the cluster manager itself. This node monitor is responsible for monitoring the services which
    are running on the node and including this information in heartbeats. The node monitor
    heartbeats are similar to the cluster manager heartbeats in that they are exchanged between all
    nodes, but these heartbeats contained much more detailed information. Each node then decides
    whether a particular node is healthy based on the heartbeats that it receives from the node. A
    node is only considered for automatic failover if every other node reports that it is
    unhealthy.</p><p dir="ltr">Currently the following services are monitored:</p><ul
    id="ul_kqm_kgz_pz">
    <li dir="ltr">
     <p dir="ltr"><b>Cluster Manager</b> - The cluster management process' health is assessed by
      monitoring the basic heartbeating that the cluster manager performs. If the cluster manager is
      not sending out its basic heartbeats to other nodes then it is considered unhealthy.</p>
    </li>
    <li dir="ltr">
     <p dir="ltr"><b>Data Service</b> - The health of the Data service is monitored by inspecting
      the DCP
       traffic coming out of the node. If the memcached process is not sending data or no-ops
      (internal keep-alives) along its connections then it is considered unhealthy.</p>
    </li>
   </ul>This monitoring is then used to more accurately inform the decision about whether to
   automatically failover a node. By using specific service monitoring, the chance for 'false
   positives' is greatly reduced, although it is still vulnerable to an unstable network. </section>
  <section id="RZA"><title>Smart Data Placement with Rack and Zone Awareness</title>
   <p>Couchbase Server buckets physically contain 1024 master and 0 or more replica vBuckets. The Cluster Manager master services module governs the placement of these vBuckets to maximize availability and rebalance performance.</p>
   <p>The Cluster Manager master services module calculates a vBucket map with heuristics to maximize availability and rebalance performance. The vBucket map is recalculated whenever the cluster topology changes. The following rules govern the vBucket map calculation: <ul>
    <li>Master and replica vBuckets are placed on separate nodes to protect against node failures. </li>
    <li>If a bucket is configured with more than 1 replica vBucket, each additional replica vBucket is placed on a separate node to provide better protection against node failures. </li>
    <li>If server groups are defined  for master vBuckets (such as rack and zone awareness capability),  the replica vBuckets are placed in a separate server group for better protection against rack or availability zone failures.</li>
   </ul></p>
  </section>
  <section><title>Centralized Management, Statistics, and Logging</title>
   <p>The Cluster Manager simplifies centralized management with centralized configuration management, statistics gathering and logging services. All configuration changes are managed by the orchestrator and pushed out to the other nodes to avoid configuration conflicts. </p>

   <p>In order to understand what your cluster is doing and how the cluster is performing, 
    Couchbase Server incorporates a complete set of statistical and monitoring information. 
    The statistics are accessible through all the administration interfaces - CLI ( cbstats tool), REST API, and the Couchbase Web Console.</p>

   <p> The Couchbase Web Console provides a complete suite of statistics including the built-in real-time graphing and performance data. It gives great flexibility as you (as an Administrator) can aggregate the statistics for each bucket and choose to view the statistics for the whole cluster or per node. </p>
   <p>The statistics information is grouped into categories, allowing you to identify different states and performance information within the cluster. <dl>
    <dlentry>
     <dt>Statistics on hardware resources</dt>
     <dd>Node statistics show CPU, RAM and I/O numbers on each of the servers and across your cluster as a whole. This information is useful to identify performance and loading issues on a single server.</dd>
    </dlentry>
    <dlentry>
     <dt>Statistics on vBuckets</dt>
     <dd>The vBucket statistics shows the usage and performance numbers for the vBuckets. This is useful to determine whether you need to reconfigure your buckets or add servers to improve performance.
     </dd>
    </dlentry>
    <dlentry>
     <dt>Statistics on views and indexes</dt>
     <dd>View statistics display information about individual views in your system such as number of reads from the index or view and its disk usage, so that you can monitor the effects and loading of a view on the Couchbase nodes. This information can indicate that your views need optimization, or that you need to consider defining views across multiple design documents.</dd>
    </dlentry>
    <dlentry>
     <dt>Statistics on replication (DCP, TAP, and XDCR)</dt>
     <dd>The Database Change Protocol (DCP) interface is used to monitor changes and updates to the database. DCP is widely used internally to replicate data between the nodes, for backups with <apiname>cbbackup</apiname>, to maintain views and indexes and to integrate with external products with connectors such as Elasticsearch connector, Kafka connector or the Sqoop connector. XDCR replicates data between clusters and uses DCP in conjunction with an agent that is tuned to replicate data under higher WAN latencies. <p>TAP is similar to DCP, but is a deprecated protocol. Legacy tools may still use the protocol and stats are still available through the console.</p><p>Given the central role of replication in a distributed system like Couchbase Server, identifying statistics on replication is critical. Statistics in replication help visualize the health of replication and bottlenecks in replication by displaying replication latency and pending items in replication streams. </p>
     </dd> </dlentry></dl></p>  </section>
 </body>
</topic>
