<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_cy4_tr2_xs2">
 
 <title>
  Storage
 </title>
 
 <shortdesc>
  Couchbase Server provides persistence, whereby items are stored on disk, as well as
  in memory; and reliability is thereby enhanced. 
 </shortdesc>
 
 <body>
  
  <section id="understanding-couchbase-storage">
  
  <title>
   Understanding Couchbase Storage
  </title>
  
  <p>
   Couchbase Server <i>storage</i> entails the saving and maintenance of data in
   compressed form on disk. This allows data-sets to exceed the size permitted
   by current memory-resources; since items not in memory can be
   restored to memory from disk, as needed. It also facilitates backup and restore, 
   since data lost from memory due to node-failure, can be reacquired from disk
   when the node itself is brought back to health.
  </p>
   
   <p>
    Client-interactions with the server are not blocked during the process whereby items 
    are written to and read
    from disk (other than in terms of a specific item, being restored from disk to memory,
    not being made available to the client until restoration is complete).
   </p>
   
   <p>
    Not all items are saved on disk: <i>Ephemeral</i> buckets and their items are maintained
    in memory only. See
    <xref href="../buckets-memory-and-storage/buckets.dita" scope="local" format="dita">Buckets</xref>
    for detailed information.
   </p>
   
   <p>
    Items saved to disk are always saved in compressed form. Based on bucket configuration, 
    items may be maintained in compressed form in memory also. See
    <xref href="../buckets-memory-and-storage/compression.dita" scope="local" format="dita">Compression</xref>
    for detailed information.
   </p>
   
   <p>
    Items can be removed from disk based on a configured point of expiration. See
    <xref href="../buckets-memory-and-storage/expiration.dita" scope="local" format="dita">Expiration</xref>
    for detailed information.
   </p>
   
   <p>
    For illustrations of how Couchbase Server saves new Couchbase bucket-items and updates existing, using
    both memory and storage resources, see
    <xref href="../buckets-memory-and-storage/memory-and-storage.dita" scope="local" format="dita">Memory 
     and Storage</xref>.
   </p>

  </section>
  
  <section id="threading">
   
   <title>
    Threading
   </title>
   
   <p>
    Synchronized, multi-threaded <i>readers</i> and <i>writers</i> provide simultaneous read-write operations 
    for data on disk: this simultaneity increases I/O throughput. To avoid conflict, each reader-thread or 
    writer-thread is 
    assigned to a specific subset of the 1024
    vBuckets for a given bucket. For example, the first of five reader-threads is assigned to 
    vBuckets 0, 6, 11, 16, 21, and so on; while the second is assigned to vBuckets 1, 7, 12, 17, 22, and
    so on.
   </p>
  </section>
  
  <section id="deletion">
   
   <title>
    Deletion
   </title>
   
   <p>
    
   </p>
   
  </section>
  
  <dl>
   
   <dlentry>
    <dt>Item deletion</dt>
    <dd>Items can be deleted explicitly by the client applications or deleted using an expiration flag. 
     Couchbase Server never deletes items from disk unless one of these operations are performed. 
     However, after deletion or expiration, a tombstone is maintained as the record of deletion. 
     Tombstones help communicate the deletion or the expiration to downstream components. Once all 
     downstream components have been notified, the tombstone gets purged as well. </dd>
   </dlentry>
   <dlentry id="tombstone">
    <dt>Tombstone purging</dt>
    <dd>Tombstones are records of expired or deleted items that include item keys and metadata.
      <p>Couchbase Server and other distributed databases maintain tombstones in order to provide
      eventual consistency between nodes and between clusters. Tombstones are records of expired or
      deleted items and they include the key for the item and metadata. Couchbase Server stores the
      key plus several bytes of metadata per deleted item in two structures per node. With millions
      of mutations, the space taken up by tombstones can grow quickly. This is especially the case
      if there are a large number of deletions or expired documents.</p><p>The Metadata Purge
      Interval sets frequency for a node to permanently purge metadata of deleted and expired items.
      The Metadata Purge Interval setting runs as part of auto-compaction. This helps reduce the
      storage requirement by roughly 3x times than before and also frees up space much faster.</p></dd> </dlentry>
  </dl>
  
  <section id="disk-priority"><title>Disk I/O priority</title>
   <p>Disk I/O priority enables workload priorities to be set at the bucket level.</p>
   <p>You can configure the bucket priority settings at the bucket level and set the value to be either high or low. Bucket priority settings determine whether I/O tasks for a bucket must be queued in the low or high priority task queues. Threads in the global pool poll the high priority task queues more often than the low priority task queues. When a bucket has a high priority, its I/O tasks are picked up at a higher frequency and thus, processed faster than the I/O tasks belonging to a low priority bucket.</p>
   <p>You can configure the bucket I/O priority settings during initial setup and change the
    settings later, if needed. However, changing a bucket I/O priority after the initial setup
    results in a restart of the bucket and the client connections are reset. <fig
     id="fig_hvl_jq2_2t">
     <title>Create bucket settings</title>
     <image placement="break" href="./images/create-bucket-settings.png" width="600"
      id="image_ivl_jq2_2t"/>
    </fig></p>
   <p>The previous versions of Couchbase Server, version 3.0 or earlier, required the I/O thread
    allocation per bucket to be configured manually. However, when you upgrade from a 2.x version to
    a 3.x or higher version, Couchbase Server converts an existing thread value to either a high or
    low priority based on the following criteria: <ul>
     <li>Buckets allocated six to eight (6-8) threads in Couchbase Server 2.x are marked high
      priority in bucket setting after the upgrade to 3.x or later.</li>
     <li>Buckets allocated three to five (3-5) threads in Couchbase Server 2.x are marked low
      priority in bucket settings after the upgrade to 3.x or later.</li>
    </ul></p>
   
  </section>
  <section><title>Monitoring Scheduler</title>
   <p>You can use the <cmdname>cbstats</cmdname> command with the <parmname>raw workload</parmname> option to view the status of the threads as shown in the following example.
    <codeblock># cbstats 10.5.2.54:11210 -b default raw workload
     
     ep_workload:LowPrioQ_AuxIO:InQsize:   3
     ep_workload:LowPrioQ_AuxIO:OutQsize:  0
     ep_workload:LowPrioQ_NonIO:InQsize:   33
     ep_workload:LowPrioQ_NonIO:OutQsize:  0
     ep_workload:LowPrioQ_Reader:InQsize:  12
     ep_workload:LowPrioQ_Reader:OutQsize: 0
     ep_workload:LowPrioQ_Writer:InQsize:  15
     ep_workload:LowPrioQ_Writer:OutQsize: 0
     ep_workload:num_auxio:                1
     ep_workload:num_nonio:                1
     ep_workload:num_readers:              1
     ep_workload:num_shards:               4
     ep_workload:num_sleepers:             4
     ep_workload:num_writers:              1
     ep_workload:ready_tasks:              0
     ep_workload:shard0_locked:            false
     ep_workload:shard0_pendingTasks:      0
     ep_workload:shard1_locked:            false
     ep_workload:shard1_pendingTasks:      0
     ep_workload:shard2_locked:            false
     ep_workload:shard2_pendingTasks:      0
     ep_workload:shard3_locked:            false
     ep_workload:shard3_pendingTasks:      0 </codeblock> </p>
  </section>
  
  <section><title>High Performance Storage</title>
   <p>The scheduler and the shared thread pool provide high performance storage to the Couchbase Server.</p>
   <dl>
    <dlentry>
     <dt>Scheduler</dt>
     <dd>The scheduler is responsible for managing a shared thread-pool and providing a fair
      allocation of resources to the jobs waiting to execute in the vBucket engine. Shared thread
      pool services requests across all buckets. <p>As an administrator, you can govern the
       allocation of resources by configuring a bucketâ€™s disk I/O prioritization setting to be
       either high or low.</p></dd></dlentry>
    <dlentry>
     <dt>Shared thread pool</dt>
     <dd>A shared thread pool is a collection of threads which are shared across multiple buckets for long running operations such as disk I/O. Each node in the cluster has a thread pool that is shared across multiple vBuckets on the node. Based on the number of CPU cores on a node, the database engine spawns and allocates threads when a node instance starts up.
      <p>Using a shared thread pool provides the following benefits: <ul>
       <li>Better parallelism for worker threads with more efficient I/O resource management. </li>
       <li>Better system scalability with more buckets being serviced with fewer worker threads.</li>
       <li>Availability of task priority if the disk bucket I/O priority setting is used.</li>
      </ul></p></dd> </dlentry>
   </dl>
  </section>
  
 </body>
</topic>
