<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_x54_dmj_vs2">
 
 <title>
  Memory
 </title>
 
 <shortdesc>
  Couchbase Server memory-management ensures high performance and scalability.
 </shortdesc>
 
 <body>
  
  <section id="service-memory-quotas">
   
   <title>Service Memory Quotas</title>
   
   <p>
    Memory-quota allocation on Couchbase Server occurs <i>per service</i> (except in the case
    of the <i>Query Service</i>, which does not require a specific allocation). This allows
    the availability of memory-resources to be tuned according to the assignment of services,
    node by node. Note that the Data Service must run on every node; and that quotas for buckets,
    specified at the time of bucket-creation, are subtracted 
    from the quota allocated to the <i>Data Service</i>.
   </p>
   
   <p>
    The memory-quota allocation specified for a given service applies to every instance of that
    service across the cluster. For example, if 2048 MB is specified for the Analytics Service,
    and the Analytics Service is run on three of a cluster's five nodes, each of the three
    instances of the service is duly allocated 2048 MB. Note that it is not possible to have
    different memory allocations across multiple instances of the same service within a single cluster.
   </p>
   
   <p>
    By default, Couchbase Server allows 80% of a node's total available memory to be allocated to 
    the server and its services.
    Consequently, if a node's total available memory is 100 GB, any attempt to allocate memory beyond 80 GB produces
    an error. Therefore, when a cluster is being assembled, available memory resources should be carefully calculated
    and cross-referenced with anticipated
    service-requirements. Note, for example, that if an initial cluster-node 
    with 100 GB of memory is assigned 
    the Data Service only, and the Data Service memory
    allocation is specified as 80 GB, an error results from any attempt to add to the cluster a new
    node with only 90 GB of total memory: since the Data Service is now bound to run with
    a memory allocation of 80 GB on every node, and the new node is unable to accommodate this
    allocation within 80% of its own available memory. (This situation might be resolved by lowering
    the quota for the Data Service, by means of the <b>Service Memory Quotas</b> panel, described below.)
   </p>
   
   <p>
    When a node is added to a cluster, the <i>Default Configuration</i>, as established by the set-up of
    the first node in the cluster, is available: this covers all configurable elements, including memory
    quotas. However, if insufficient memory for the default configuration is available on the new node,  
    the default configuration is prohibited: in such cases, settings
    for the new node can be custom-configured instead; allowing a subset of services to be specified, such
    as can be accommodated by the available memory of the new node. 
   </p>
   
   <p>
    If, when the initial node of a cluster is set up, only a subset of services is assigned, additional
    nodes might subsequently be added, in order to host additional services: in which case, each new service can be given
    its initial memory allocation as its node is added.
   </p>
   
   <p>
    The <b>Service Memory Quotas</b> panel on the <b>Settings</b> screen of Couchbase Web Console lists all 
    services running on the cluster, and specifies
    the memory allocation for each.
    The panel is interactive, and allows the memory allocations to be changed and saved. If a modification
    attemptedly goes above the maximum allocation supported by any node in the cluster, or below
    minimum quotas that are either imposed by default (see below) or are the result of existing usage (as
    might be the case with Data Service memory-resources, allocated to defined buckets),
    a notification 
    of the error is displayed, and the modification is disallowed. See 
    <xref href="../../settings/settings.dita" scope="local" format="dita">Settings</xref> for further information.
   </p>
   
   <p>
    Each service (except the Query Service) has a minimum required memory-quota. These are
    as follows:
   </p>
   
   <table frame="all" rowsep="1" colsep="1" id="memory_quota_mimumums">

    <tgroup cols="2" align="left">
     <colspec colname="c1" colnum="1"/>
     <colspec colname="c2" colnum="2"/>
     <thead>
      <row>
       <entry>Service</entry>
       <entry>Minimum Memory Quota (in MB)</entry>
      </row>
     </thead>
     <tbody>
      
      <row>
       <entry>Data</entry>
       <entry>256</entry>
      </row>
      
      <row>
       <entry>Index</entry>
       <entry>256</entry>
      </row>
      
      <row>
       <entry>Search</entry>
       <entry>256</entry>
      </row>
      
      <row>
       <entry>Analytics</entry>
       <entry>1024</entry>
      </row>
      
      <row>
       <entry>Eventing</entry>
       <entry>256</entry>
      </row>
      
     </tbody>
    </tgroup>
   </table>
   
   
  </section>

  
  <section>
   
   <title>
    Initialization and Warmup
   </title>
   
   <p>
    When Couchbase Server is restarted on a node, the node goes through a <i>warmup process</i> 
    before it starts handling data requests again. During this warmup process, data 
    on disk is sequentially reloaded into memory. The time required
    depends on the size and configuration of the system, the amount of data persisted on
    the node, and the ejection policy configured for the buckets. 
   </p>
    
   <p>
    Frequently used items are
    identified via a scanner process, which examines an <i>access log</i>, and obtains the 
    appropriate keys. The corresponding items are then loaded with priority. The scanner
    process is configurable, via the CLI utility <codeph>cbepctl flush_param</codeph>.
    This utility also provides parameters, <codeph>warmup_min_memory_threshold</codeph> and 
    <codeph>warmup_min_item_threshold</codeph>, which can be used to schedule the resumption of traffic
    before all items have been reloaded into memory.
   </p>c
   
  </section>
  
  <section id="full-ejection">
   
   <title>
    Ejection
   </title>
   
   <p>
    If a bucket's memory quota is exceeded, items may be <i>ejected</i> from the bucket by the
    Data Service. Different
    <i>ejection methods</i> are available, and are configured for a bucket on its definition. Note that in
    some cases, ejection is configured not to occur.
    For detailed information, see
    <xref href="../../architecture/core-data-access-buckets.dita" scope="local" format="dita">Buckets</xref>.
   </p>
   
   <p>
    For each bucket, available memory is managed according to two <i>watermarks</i>, which are
    <codeph>mem_low_wat</codeph> and <codeph>mem_high_wat</codeph>.
    If data is continuously loaded into the bucket, its quantity is eventually at the point indicated by the
    <codeph>mem_low_wat</codeph> watermark. No action is yet taken. Then, eventually, after still more data has been
    loaded, its quantity eventually reaches the point indicated by the <codeph>mem_high_wat</codeph> watermark. 
    If, based on the bucket's configuration, items can be ejected from the bucket, 
    the Data Service now schedules a background job, referred to as the <i>item pager</i>, which
    ejects items from the bucket until
    the quantity of data has decreased to the <codeph>mem_low_wat</codeph> watermark. 
    If the rate at which data continues to be loaded is greater than that at which it is being ejected, or
    if ejection cannot be performed, the system provides notification of insufficient memory, until
    sufficient memory becomes or is made available.
   </p> 
   
   <p>
    The relationship of <codeph>mem_low_wat</codeph> and <codeph>mem_high_wat</codeph> to the 
    bucket's overall memory quota is illustrated as follows:
   </p>
    
    <p>
     <image href="./images/tunableMemory.png" width="400" id="tunable_memory"/>
    </p>
   
  </section>
   
   <section>
    <title>
     Expiration
    </title>
   <p>
      The expiration process in Couchbase Server uses the metadata in RAM to quickly scan for
      items that have expired and later removes them from disk. This process is known as <i>expiry pager</i> 
      and runs every 60 minutes by default.
   </p>
   </section>  
  
  <section>
   
   <title>
    Not Recently Used (NRU) Items
   </title>
   
   <p>
    All items in the server contain metadata indicating whether or not the item has been recently accessed. 
    This metadata is known as not-recently-used (NRU). If an item has not been recently used, 
    then the item is a candidate for ejection. When data in the cache exceeds the high water mark 
    (mem_high_wat), the server evicts items from RAM.
   </p>
   
   <p>
    Couchbase Server provides two NRU bits per item and also provides a replication protocol that can 
    propagate items that are frequently read, but not mutated often.
   </p>
   
   <p>
    NRUs are decremented or incremented by server processes to indicate an item that is more
    frequently or less frequently used. The following table lists the bit values with the
    corresponding scores and statuses: 
    
    <table frame="all" rowsep="1" colsep="1"
     id="table_ekt_2yz_xs">
     <title>Scoring for NRU bit values</title>
     <tgroup cols="4" align="left">
      <colspec colname="c1" colnum="1"/>
      <colspec colname="c2" colnum="2"/>
      <colspec colname="c3" colnum="3"/>
      <colspec colname="c4" colnum="4"/>
      <thead>
       <row>
        <entry>Binary NRU</entry>
        <entry>Score</entry>
        <entry>Access pattern</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>00</entry>
        <entry>0</entry>
        <entry>Set by write access to 00. Decremented by read access or no access.</entry>
        <entry>Most heavily used item.</entry>
       </row>
       <row>
        <entry>01</entry>
        <entry>1</entry>
        <entry>Decremented by read access.</entry>
        <entry>Frequently accessed item.</entry>
       </row>
       <row>
        <entry>10</entry>
        <entry>2</entry>
        <entry>Initial value or decremented by read access.</entry>
        <entry>Default value for new items.</entry>
       </row>
       <row>
        <entry>11</entry>
        <entry>3</entry>
        <entry>Incremented by item pager for eviction.</entry>
        <entry>Less frequently used item.</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </p>
   
   <p>
    There are two processes that change the NRU for an item:
    
    <ul>
     <li>
      When a client reads or writes an item, the server decrements NRU and lowers the item's score.
     </li>
     
     <li>
      A daily process which creates a list of frequently-used items in RAM. After the completion of this process, 
      the server increments one of the NRU bits. 
     </li>
     
    </ul>
    
    Because these two processes change NRUs, they play an important role in identifying the candidate items for ejection. </p>
   <p>You can configure the Couchbase Server settings to change the behavior during ejection. For example, you can specify the percentage of RAM to be consumed before items are ejected, or specify whether ejection should occur more frequently on replicated data than on original data. Couchbase recommends that the default settings be used.</p>
  </section>
  <section><title>Understanding the Item Pager</title>
   <p>The item pager process runs periodically to remove documents from RAM. When the amount of RAM
    used by items reaches the high water mark (upper threshold), both active and replica data are
    ejected until the amount of RAM consumed (memory usage) reaches the low water mark (lower
    threshold). Using the default settings, active documents have a 40% chance of eviction while
    replica documents have a 60% chance of eviction. Both the high water mark and low water mark are
    expressed as a percentage amount of RAM, such as 80%.</p>
   <p>You can change the high water mark and low water mark settings for a node by specifying a
    percentage amount of RAM, for example, 80%. Couchbase recommends that you use the following
    default settings: <table frame="all" rowsep="1" colsep="1" id="table_pnj_x21_ys">
     <title>Default setting for RAM water marks</title>
     <tgroup cols="3" align="left">
      <colspec colname="c1" colnum="1"/>
      <colspec colname="c2" colnum="2"/>
      <colspec colname="c3" colnum="3"/>
      <thead>
       <row>
        <entry>Version</entry>
        <entry>High water mark</entry>
        <entry>Low water mark</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>2.0</entry>
        <entry>75%</entry>
        <entry>60%</entry>
       </row>
       <row>
        <entry>2.0.1 and higher</entry>
        <entry>85%</entry>
        <entry>75%</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </p>
   
   <p>
    The item pager ejects items from RAM in two phases: 
    
    <ol>
     
     <li>
      Eject items based on NRU: The item pager scans NRU for items, creates a list of 
      items with a NRU score 3, and ejects all the identified items. It then 
      checks the RAM usage and repeats the process if the usage is still above 
      the low water mark. 
     </li>
     
     <li>
      Eject items based on algorithm: The item pager increments the NRU of all items by 1. For
      every item whose NRU is equal to 3, it generates a random number. If the random number for an
      item is greater than a specified probability, it ejects the item from RAM. The probability is
      based on the current memory usage, low water mark, and whether a vBucket is in an active or
      replica state. If a vBucket is in an active state, the probability of ejection is lower than
      if the vBucket is in a replica state. 
      
      <table frame="all" rowsep="1" colsep="1"
       id="table_qpb_hg1_ys">
       <title>Probability of ejection based on active vBuckets versus replica vBuckets (using
        default settings)</title>
       <tgroup cols="2" align="left">
        <colspec colname="c1" colnum="1"/>
        <colspec colname="c2" colnum="2"/>
        <thead>
         <row>
          <entry>Active vBucket</entry>
          <entry>Replica vBucket</entry>
         </row>
        </thead>
        <tbody>
         <row>
          <entry>40%</entry>
          <entry>60%</entry>
         </row>
        </tbody>
       </tgroup>
      </table></li>
    </ol>
   </p>
  </section>
  
  <section>
   <title>
    Active Memory Defragmenter
   </title>
   
   <p>
    Over time, the memory used by the managed cache of a running Couchbase Server can become 
    fragmented. The storage engine now includes an <i>Active Defragmenter</i> task to 
    defragment cache memory.
   </p>
   
   <p>
    Cache fragmentation is a side-effect of how Couchbase Server organizes cache memory to maximize 
    performance. Each page in the cache is typically responsible for holding documents of a specific 
    size range. Over time, if memory pages assigned to a specific size range become sparsely populated 
    (due to documents of that size being ejected or items changing in size), then the unused space in 
    those pages cannot be used for documents of other sizes until a complete page is free and that page 
    is re-assigned to a new size. Such effects are highly workload dependent and can result in memory 
    that cannot be used efficiently by the managed cache. 
   </p> 
   
   <p>
    The Active Memory Defragmenter attempts to address any fragmentation by periodically scanning the 
    cache to identify pages which are sparsely used, and repacking the items stored on those pages to 
    free up <i>whole</i> pages.
   </p>
   
  </section>
  
  
  <section>
   
   <title>
    Bloom Filters
   </title>
   
   <p>
    A Bloom filter is a probabilistic data structure used to test whether an element is a
   member of a set. False positive matches are possible, but false negatives are not. This means a
   query returns either "possibly in set" or "definitely not in set". It is a bit array with a
   predefined size that is calculated based on the expected number of items and the probability of
   false positives or the probability of finding a key that doesn't exist. Bloom filter
   significantly improves the performance of full ejection scenarios and XDCR.  
   </p>

   <p>
    In the full ejection mode, the key and metadata are evicted along with the value. Therefore,
    if a key is non resident, there is no way of knowing if a key exists or not, without accessing
    the disk. In such a scenario, if a client issues a lot of GETs on keys that may not even exist
    in server, Bloom filters help eliminate many unnecessary disk accesses. Similarly for XDCR,
    when we set up remote replication to a brand new cluster, we would be able to avoid many
    unnecessary GetMeta-disk-fetches with the help of the bloom filter.
   </p>
   
   <p>
    With Bloom filters, the probability of false positives decreases as the size of the array
    increases and increases as the number of inserted elements increases. Based on the algorithm
    that takes into account the number of keys and the probability of false positives, you can
    estimate the size of the Bloom filter and the number of bits to store each key. 
   </p>
   
   <p>
    For value eviction only the deleted keys will be stored, while in case of full eviction both
    the deleted keys and non-resident items will be stored. 
   </p>
   
   <p>
    The algorithm calculates the almost exact probability of false positives, including the
    number of hash functions (<codeph>k</codeph>), size of the bit array (<codeph>m</codeph>), and
    the number of inserted elements (<codeph>n</codeph>):
   </p>
   
   <codeblock outputclass="language-c">k = m/n (ln 2)</codeblock>
   
   <p>
    You can expect an increase in memory usage or memory overhead while using the Bloom filter:
    
    <table frame="all" rowsep="1" colsep="1" id="table_dkx_rvg_vq">
     <title>Memory overhead for Bloom filter use</title>
     <tgroup cols="3" align="left">
      <colspec colname="c1" colnum="1" colwidth="1.0*"/>
      <colspec colname="c2" colnum="2" colwidth="1.0*"/>
      <colspec colname="c3" colnum="3" colwidth="1.0*"/>
      <thead>
       <row>
        <entry>False positive probability</entry>
        <entry>0.01</entry>
        <entry>0.05</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Estimated number of keys</entry>
        <entry>10,000.000 (=> =10,000 keys per vBucket)</entry>
        <entry>10,000.000 (=> =10,000 keys per vBucket)</entry>
       </row>
       <row>
        <entry>Number of bits per key in the filter</entry>
        <entry>7 bits</entry>
        <entry>4 bits</entry>
       </row>
       <row>
        <entry>Size of the Bloom filter to fit the estimated keys with desired false positive
         probability</entry>
        <entry>95851 bits (=> =12 KB per vBucket) (=> =12 MB for 1024 vBuckets)</entry>
        <entry>62353 bits (=> =8 KB per vBucket) (=> =8 MB for 1024 vBuckets)</entry>
       </row>
      </tbody>
     </tgroup>
    </table></p> 
   
   <p>
    In a case of full eviction, you will not know whether an item exists in the memory until you
    perform a background fetch. Therefore, use of the Bloom filter helps to avoid unnecessary
    background fetches and improves latency. 
   </p>
   
   <p>
    For more information about working set management and eviction, see <xref href="./db-engine-architecture.dita"></xref></p>
  </section>
 </body>
</topic>
