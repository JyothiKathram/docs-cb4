<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_x54_dmj_vs2">
 
 <title>
  Memory
 </title>
 
 <shortdesc>
  Couchbase Server memory-management ensures high performance and scalability.
 </shortdesc>
 
 <body>
  
  <section id="service-memory-quotas">
   
   <title>Service Memory Quotas</title>
   
   <p>
    Memory-quota allocation on Couchbase Server occurs <i>per service</i> (except in the case
    of the <i>Query Service</i>, which does not require a specific allocation). This allows
    the availability of memory-resources to be tuned according to the assignment of services,
    node by node. Note that the Data Service must run on every node; and that quotas for buckets,
    specified at the time of bucket-creation, are subtracted 
    from the quota allocated to the <i>Data Service</i>.
   </p>
   
   <p>
    The memory-quota allocation specified for a given service applies to every instance of that
    service across the cluster. For example, if 2048 MB is specified for the Analytics Service,
    and the Analytics Service is run on three of a cluster's five nodes, each of the three
    instances of the service is duly allocated 2048 MB. Note that it is not possible to have
    different memory allocations across multiple instances of the same service within a single cluster.
   </p>
   
   <p>
    By default, Couchbase Server allows 80% of a node's total available memory to be allocated to 
    the server and its services.
    Consequently, if a node's total available memory is 100 GB, any attempt to allocate memory beyond 80 GB produces
    an error. Therefore, when a cluster is being assembled, available memory resources should be carefully calculated
    and cross-referenced with anticipated
    service-requirements. Note, for example, that if an initial cluster-node 
    with 100 GB of memory is assigned 
    the Data Service only, and the Data Service memory
    allocation is specified as 80 GB, an error results from any attempt to add to the cluster a new
    node with only 90 GB of total memory: since the Data Service is now bound to run with
    a memory allocation of 80 GB on every node, and the new node is unable to accommodate this
    allocation within 80% of its own available memory. (This situation might be resolved by lowering
    the quota for the Data Service, by means of the <b>Service Memory Quotas</b> panel, described below.)
   </p>
   
   <p>
    Instructions on how to allocate memory quotas to services when initializing a new cluster can be
    found in the section
    <xref href="../../install/init-setup.dita#topic12527/configure-couchbase-server" scope="local" format="dita">Configure Couchbase Server</xref>,
    on the page
    <xref href="../../install/init-setup.dita" scope="local" format="dita">Initializing the Cluster</xref>.
   </p>
   
   <p>
    When a node is added to a cluster, the <i>Default Configuration</i>, as established by the set-up of
    the first node in the cluster, is available: this covers all configurable elements, including memory
    quotas. However, if insufficient memory for the default configuration is available on the new node,  
    the default configuration is prohibited: in such cases, settings
    for the new node can be custom-configured instead; allowing a subset of services to be specified, such
    as can be accommodated by the available memory of the new node. 
   </p>
   
   <p>
    If, when the initial node of a cluster is set up, only a subset of services is assigned, additional
    nodes might subsequently be added, in order to host additional services: in which case, each new service can be given
    its initial memory allocation as its node is added.
   </p>
   
   <p>
    Instructions on how to add a new node to a cluster, determine which services be retained, and
    add new services, can
    be found in the section 
    <xref href="../../install/init-setup.dita#topic12527/join-an-existing-cluster" scope="local" format="dita">Join an Existing Cluster</xref>,
    on the page
    <xref href="../../install/init-setup.dita" scope="local" format="dita">Initializing the Cluster</xref>.
   </p>
   
   <p>
    The <b>Service Memory Quotas</b> panel on the <b>Settings</b> screen of Couchbase Web Console lists all 
    services running on the cluster, and specifies
    the memory allocation for each.
    The panel is interactive, and allows the memory allocations to be changed and saved. If a modification
    attemptedly goes above the maximum allocation supported by any node in the cluster, or below
    minimum quotas that are either imposed by default (see below) or are the result of existing usage (as
    might be the case with Data Service memory-resources, allocated to defined buckets),
    a notification 
    of the error is displayed, and the modification is disallowed. See 
    <xref href="../../settings/cluster-settings.dita" scope="local" format="dita">Configure Cluster Settings</xref> 
    for further information.
   </p>
   
   <p>
    Each service (except the Query Service) has a minimum required memory-quota. These are
    as follows:
   </p>
   
   <table frame="all" rowsep="1" colsep="1" id="memory_quota_mimumums">

    <tgroup cols="2" align="left">
     <colspec colname="c1" colnum="1"/>
     <colspec colname="c2" colnum="2"/>
     <thead>
      <row>
       <entry>Service</entry>
       <entry>Minimum Memory Quota (in MB)</entry>
      </row>
     </thead>
     <tbody>
      
      <row>
       <entry>Data</entry>
       <entry>256</entry>
      </row>
      
      <row>
       <entry>Index</entry>
       <entry>256</entry>
      </row>
      
      <row>
       <entry>Search</entry>
       <entry>256</entry>
      </row>
      
      <row>
       <entry>Analytics</entry>
       <entry>1024</entry>
      </row>
      
      <row>
       <entry>Eventing</entry>
       <entry>256</entry>
      </row>
      
     </tbody>
    </tgroup>
   </table>
   
   
  </section>

  
  <section id="initialization-and-warmup">
   
   <title>
    Initialization and Warmup
   </title>
   
   <p>
    When Couchbase Server is restarted on a node, the node goes through a <i>warmup process</i> 
    before it starts handling data requests again. During this warmup process, data 
    on disk is sequentially reloaded into memory. The time required
    depends on the size and configuration of the system, the amount of data persisted on
    the node, and the ejection policy configured for the buckets. 
   </p>
    
   <p>
    Frequently used items are
    identified via a scanner process, which examines an <i>access log</i>, and obtains the 
    appropriate keys. The corresponding items are then loaded with priority. The scanner
    process is configurable, via the CLI utility <codeph>cbepctl flush_param</codeph>.
    This utility also provides parameters, <codeph>warmup_min_memory_threshold</codeph> and 
    <codeph>warmup_min_item_threshold</codeph>, which can be used to schedule the resumption of traffic
    before all items have been reloaded into memory.
   </p>
   
  </section>
  
  <section id="ejection">
   
   <title>
    Ejection
   </title>
   
   <p>
    If a bucket's memory quota is exceeded, items may be <i>ejected</i> from the bucket by the
    Data Service. Different
    <i>ejection methods</i> are available, and are configured per bucket. Note that in
    some cases, ejection is configured <i>not</i> to occur.
    For detailed information, see
    <xref href="../../architecture/core-data-access-buckets.dita" scope="local" format="dita">Buckets</xref>.
   </p>
   
   <p>
    For each bucket, available memory is managed according to two <i>watermarks</i>, which are
    <codeph>mem_low_wat</codeph> and <codeph>mem_high_wat</codeph>.
    If data is continuously loaded into the bucket, its quantity eventually 
    increases to the value indicated by the
    <codeph>mem_low_wat</codeph> watermark. No action is yet taken. Then, as still more data has been
    loaded, its quantity 
    increases to the value indicated by the <codeph>mem_high_wat</codeph> watermark. 
    If, based on the bucket's configuration, items can be ejected from the bucket, 
    the Data Service now schedules the <i>item pager</i> (described in detail below), which
    ejects items from the bucket until
    the quantity of data has decreased to the <codeph>mem_low_wat</codeph> watermark. 
    If the rate at which data continues to be loaded is greater than that at which it is being ejected, or
    if ejection cannot be performed, the system provides notification of insufficient memory, until
    sufficient memory becomes or is made available.
   </p> 
   
   <p>
    The relationship of <codeph>mem_low_wat</codeph> and <codeph>mem_high_wat</codeph> to the 
    bucket's overall memory quota is illustrated as follows:
   </p>
    
    <p>
     <image href="./images/tunableMemory.png" width="400" id="tunable_memory"/>
    </p>
   
  </section>

  <section>
   
   <title>
    Not Recently Used
   </title>
   
   <p>
    All items in the server contain metadata that indicates whether or not the item has been recently accessed. 
    This metadata is known as <i>Not Recently Used</i> (NRU). If an item has not been recently used, 
    the item is a candidate for ejection. When data in the cache exceeds the high water mark, the 
    server ejects items from memory.
   </p>
   
   <p>
    The NRU metadata field
    consists of two bits. A replication protocol is provided for
    propagating items that are frequently read, but are not often mutated.
    The NRU value changes in the following circumstances:
   </p>
   
   <ul>
    <li>
     A client reads or writes an item. The server correspondingly decrements the NRU value,
     and so lowers the item's score.
    </li>
    
    <li>
     The <i>item pager</i> runs, and determines which items are frequently used. On completion, 
     the item pager increments the NRU value of less frequently used items. 
    </li>
    
   </ul>
    
    <table frame="all" rowsep="1" colsep="1"
     id="table_ekt_2yz_xs">
     <tgroup cols="4" align="left">
      <colspec colname="c1" colnum="1" colwidth="0.4*"/>
       <colspec colname="c2" colnum="2" colwidth="0.4*"/>
        <colspec colname="c3" colnum="3" colwidth="1.0*"/>
         <colspec colname="c4" colnum="4" colwidth="1.0*"/>
      <thead>
       <row>
        <entry>Binary NRU</entry>
        <entry>Score</entry>
        <entry>Access Pattern</entry>
        <entry>Description</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>00</entry>
        <entry>0</entry>
        <entry>Set by write access to 00. Decremented by read access or no access.</entry>
        <entry>Most heavily used item.</entry>
       </row>
       <row>
        <entry>01</entry>
        <entry>1</entry>
        <entry>Decremented by read access.</entry>
        <entry>Frequently accessed item.</entry>
       </row>
       <row>
        <entry>10</entry>
        <entry>2</entry>
        <entry>Initial value or decremented by read access.</entry>
        <entry>Default value for new items.</entry>
       </row>
       <row>
        <entry>11</entry>
        <entry>3</entry>
        <entry>Incremented by item pager for eviction.</entry>
        <entry>Less frequently used item.</entry>
       </row>
      </tbody>
     </tgroup>
    </table>

   <p>
    Couchbase Server settings can be configured to specify the percentage of memory to be consumed 
    before items are ejected, 
    and to specify whether ejection should occur more frequently on replicated data than on original 
    data.
   </p>
    
  </section>
  
  <section id="item-pager">
   
   <title>
    Item Pager
   </title>
   
   <p>
    The item pager runs periodically, to remove items from memory. When the amount of
    a bucket's memory that is
    used by its items reaches the high water mark, both active and replica data are
    ejected, until memory usage for the bucket reaches the low water mark. Using the default settings,
    active items have a 40% chance of eviction, while
    replica items have a 60% chance of eviction. Both the high water mark and low water mark are
    expressed as a percentage amount of memory, such as 80%. Settings can be
    custom-specified. The following table shows the recommended, default values:
   </p>
   
   <table>
    <tgroup cols="3" align="left">
     <colspec colname="c1" colnum="1"/>
     <colspec colname="c2" colnum="2"/>
     <colspec colname="c3" colnum="3"/>
     <thead>
      <row>
       <entry>Version</entry>
       <entry>High Water Mark</entry>
       <entry>Low Water Mark</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>2.0</entry>
       <entry>75%</entry>
       <entry>60%</entry>
      </row>
      <row>
       <entry>2.0.1 and higher</entry>
       <entry>85%</entry>
       <entry>75%</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   
   
   <p>
    The item pager ejects items in two phases: 
   </p>
   
   <ul>
    
    <li>
     <i>Based on NRU</i>: NRU-values are scanned, and a list is created of 
     items with a score of 3: items so identified are ejected. Memory usage 
     is checked, and the process repeated if usage is still above 
     the low water mark. 
    </li>
    
    <li>
     <i>Based on algorithm</i>: The NRU of all items is incremented by 1. Then, for
     every item whose NRU is equal to 3, a random number is generated. If an item's number
     is greater than a specified probability, the item is ejected. The probability is
     based on current memory usage, the low water mark, and whether a vBucket is in an active or
     replica state. If a vBucket is in an active state, the probability of ejection is lower than
     if the vBucket is in a replica state. 
     
     <table frame="all" rowsep="1" colsep="1" id="table_qpb_hg1_ys2">
      <tgroup cols="2" align="left">
       <colspec colname="c1" colnum="1"/>
       <colspec colname="c2" colnum="2"/>
       <thead>
        <row>
         <entry>Active vBucket</entry>
         <entry>Replica vBucket</entry>
        </row>
       </thead>
       <tbody>
        <row>
         <entry>40%</entry>
         <entry>60%</entry>
        </row>
       </tbody>
      </tgroup>
     </table></li>
   </ul>
   
  </section>
  
  <section id="expiry-pager">
   
   <title>
    Expiry Pager
   </title>
   <p>
    Scans for items that have expired, and erases them from memory and disk; after
    which, a <i>tombstone</i> remains for a default period of 3 days. The expiry pager runs
    every 60 minutes by default: for information on changing the interval, see <codeph>cbepctl</codeph>
    <xref href="../../cli/cbepctl/set-flush_param.dita" scope="local" format="dita">set flush_param</xref>.
    For more information on item-deletion and tombstones, see 
    <xref href="../../buckets-memory-and-storage/expiration.dita" scope="local" format="dita">Expiration</xref>.
   </p>
  </section>  
  
  <section id="active-memory-defragmenter">
   
   <title>
    Active Memory Defragmenter
   </title>
   
   <p>
    Over time, Couchbase Server memory can become 
    fragmented.
    Each page in memory is typically responsible for holding documents of a specific 
    size-range. Over time, if memory pages assigned to a specific size-range become sparsely populated 
    (due to documents of that size being ejected, or items changing in size), the unused space in 
    those pages cannot be used for documents of other sizes, until a complete page is free, and that page 
    is re-assigned to a new size. Such effects, which are highly workload dependent, may result in memory 
    that cannot be used efficiently. 
   </p> 
   
   <p>
    Couchbase Server provides an <i>Active Memory Defragmenter</i>, which periodically scans the 
    cache, to identify pages that are sparsely used. It then repacks the items on those pages, to 
    free up space.
   </p>
   
  </section>
  
  <section id="bloom-filters">
   
   <title>
    Bloom Filters
   </title>
   
   <p>
    By default, Couchbase Server uses 
    <xref href="https://en.wikipedia.org/wiki/Bloom_filter" scope="external" format="html">Bloom Filters</xref> 
    to minimize on-disk searches for non-existent
    items: non-existence can be determined without the disk needing to be searched.
    This allows high performance to be maintained in the contexts of Full-Value Ejection and XDCR, when
    key and metadata have been removed from memory, and may or may not continue to exist on disk.
   </p>
   
   <p>
    By means of a <i>Bloom Filter</i> an
    item can be determined <i>definitely not to be</i>, but cannot be determined <i>definitely to be</i>
    on disk. In consequence, the information provided by Bloom Filters can minimize but cannot absolutely 
    prevent on-disk searches for non-existent keys.
   </p>
    
   <p>
    When an item is created, its key is hashed, and stored in the Bloom Filter. The hash is never 
    removed from the Bloom Filter (thus, even if the item and key are entirely deleted from both memory and disk, the corresponding
    hash is retained by the Bloom Filter). When an item's key is searched on, and proves not to be in memory, the Bloom Filter is
    immediately searched for the corresponding hash. If the hash is <i>not</i> contained by
    the Bloom Filter, the item is thereby determined not to be on disk, and no disk-search is necessary. If the 
    hash <i>is</i> contained by the Bloom Filter, the item may or may not be on disk, and disk-search is required.
   </p>
   
   <p>
    Bloom Filters entail some degree of memory-overhead, as is indicated by the following table:
   </p>
    
    <table frame="all" rowsep="1" colsep="1" id="table_dkx_rvg_vq2">
     
     <tgroup cols="3" align="left">
      <colspec colname="c1" colnum="1" colwidth="1.0*"/>
      <colspec colname="c2" colnum="2" colwidth="1.0*"/>
      <colspec colname="c3" colnum="3" colwidth="1.0*"/>
      <thead>
       <row>
        <entry>False positive probability</entry>
        <entry>0.01</entry>
        <entry>0.05</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Estimated number of keys</entry>
        <entry>10,000.000 (=> =10,000 keys per vBucket)</entry>
        <entry>10,000.000 (=> =10,000 keys per vBucket)</entry>
       </row>
       <row>
        <entry>Number of bits per key in the filter</entry>
        <entry>7 bits</entry>
        <entry>4 bits</entry>
       </row>
       <row>
        <entry>Size of the Bloom Filter to fit the estimated keys with desired false positive
         probability</entry>
        <entry>95851 bits (=> =12 KB per vBucket) (=> =12 MB for 1024 vBuckets)</entry>
        <entry>62353 bits (=> =8 KB per vBucket) (=> =8 MB for 1024 vBuckets)</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   
  </section>
 </body>
</topic>
